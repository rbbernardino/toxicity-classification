{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport random\nimport re, os\nimport nltk\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.simplefilter(\"ignore\", UserWarning)\nprint(os.listdir(\"../input\"))\n% matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"text_all = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_str(string):\n    # split \"he'll\" and punctuation\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    \n    # remove repeated spaces\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n\n    # remove html tags, numbers, ' and _\n    cleanr = re.compile('<.*?>')\n    string = re.sub(r'\\d+', '', string)\n    string = re.sub(cleanr, '', string)\n    string = re.sub(\"'\", '', string)\n    string = string.replace('_', '')\n\n    # fix words like \"finallllly\" and \"awwwwwesome\"\n    pttrn_repchar = re.compile(r\"(.)\\1{2,}\")\n    string = pttrn_repchar.sub(r\"\\1\\1\", string)\n    \n    # TODO fix common spelling errors\n\n    # Stop words\n    #stop_words = set(stopwords.words('english'))\n    #word_list = text_to_word_sequence(string)\n    #no_stop_words = [w for w in word_list if not w in stop_words]\n    #no_stop_words = \" \".join(no_stop_words)\n    #string = no_stop_words\n\n    return string.strip().lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_str('asdf\\n  sdaf日本語sadf ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.width', 2000)\n    pd.set_option('display.float_format', '{:20,.2f}'.format)\n    pd.set_option('display.max_colwidth', -1)\n    display(x)\n    pd.reset_option('display.max_rows')\n    pd.reset_option('display.max_columns')\n    pd.reset_option('display.width')\n    pd.reset_option('display.float_format')\n    pd.reset_option('display.max_colwidth')\n\ndef hasNonASCII(s):\n    clean_str(s)\n    try:\n        s.encode(encoding='utf-8').decode('ascii')\n    except UnicodeDecodeError:\n        return False\n    else:\n        return True\n\ndef countNonASCII(s):\n    if hasNonASCII(s):\n        space_split = s.split(' ')\n        non_ascii_count = 0\n        for item in space_split:\n            if(not hasNonASCII(item)):\n                non_ascii_count += 1\n        return non_ascii_count\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check train data language"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print_full(text_all.sample(10).filter(['comment_text']))\ntext_all['ASCII'] = text_all['comment_text'].apply(hasNonASCII)\nnon_ascii_rows = text_all[~text_all['ASCII']]\nprint(\"non-ASCII characters:\", len(non_ascii_rows), \"samples\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_all['nASCII_count'] = (non_ascii_rows['comment_text']\n                            .apply(countNonASCII))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(non_ascii_rows\n        .sort_values(by=['nASCII_count'], ascending=False)\n        .head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nascii_tox_q = '(toxic + severe_toxic + obscene + threat + insult + identity_hate)>0'\ntox_nonascii = (non_ascii_rows\n                .sort_values(by=['nASCII_count'], ascending=False)\n                .query(nascii_tox_q))\nprint(\"Toxic with non-ascii chars:\", len(tox_nonascii))\nprint_full(tox_nonascii\n           .filter(['comment_text', 'nASCII_count'])\n           .head(100)\n           .sample(10)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some problems found\n- **17215** samples with non ASCII characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes = [\n    123420, # [text]\n    109029, 46638, # very long / very short\n    47648, # emojis\n    117817, # \"do-do-do-do-do\" pattern\n    144121, 87185, # some foreing words\n    126, # bunch of weird symbol\n    72146, # speratated by '•' char\n    10359, # translated text with untranslated sentences (starts with \"Translated text\")\n    147587, # almost no english, contains 'Sry for no English'\n    24515, # extremelly offensive with bunch of non-ascii characters (possibly to full AI systems)\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Test data language"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print_full(test_data.sample(10).filter(['comment_text']))\ntest_data['ASCII'] = test_data['comment_text'].apply(hasNonASCII)\ntest_non_ascii_rows = test_data[~test_data['ASCII']]\nprint(\"non-ASCII characters:\", len(test_non_ascii_rows), \"samples\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['nASCII_count'] = (test_data['comment_text']\n                            .apply(countNonASCII))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_non_ascii_rows = test_data[test_data['ASCII']]\nprint_full(test_non_ascii_rows\n        .sort_values(by=['nASCII_count'], ascending=False)\n        .head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Texto sem Pré-processamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_all['comment_text'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Texto limpo"},{"metadata":{"trusted":false},"cell_type":"code","source":"text_all['comment_text'] = text_all['comment_text'].apply(lambda x: clean_str(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_all['comment_text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"text_all.columns\nlabels = text_all.columns[2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Nuvem de palavras para cada label"},{"metadata":{"trusted":false},"cell_type":"code","source":"\nlist_words = []\nfor label  in labels:\n    text= \"\"\n    for comment, li in zip(text_all['comment_text'], text_all[label]):\n            if li == 1:\n                text += \" \"+comment\n    print(label)\n    wordcloud = WordCloud(max_font_size=100, max_words=1000000, background_color=\"white\").generate(text)\n    plt.figure()\n    plt.imshow(wordcloud,interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n    list_words.append(wordcloud.words_)\n                \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gráfico com as palavras que mais aparecem para cada Label"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0;\nfor label in labels:\n\n    words = list(list_words[i].keys())\n    frequencia = list(list_words[i].values())\n\n    print(label)\n    get_colors = lambda n: list(map(lambda i: \"#\" + \"%06x\" % random.randint(0, 0xFFFFFF),range(30)))\n    plt.figure(figsize=(15,10))\n    plt.bar(words[:30], frequencia[:30], color=get_colors(30))\n\n    plt.xticks(rotation=50)\n    plt.xlabel(\"Palavras\")\n    plt.ylabel(\"Frequência\")\n    plt.show()\n    i += 1","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}